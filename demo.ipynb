{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About\n",
    "\n",
    "It's difficult to use functions in jupyter notebook, since we want different steps to be in different cells, so one of the main functions of this module is to emulate a function like scope of the variables - which get destroyed at the end of the experiment. Some extra magic is added to reclaim GPU and General RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyexperiments import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and preload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_gpu(n): return torch.ones((n, n)).cuda()\n",
    "def consume_cpu(n): return np.ones((n, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pytorch`'s CUDA machinery seems to require ~0.5GB GPU RAM, and ~2GB of RAM upon its first use, and it's not shared between processes. So if you use pytorch w/ CUDA your GPU card is 0.5GB smaller from the get going, and multiply that by the number of concurrent processes. \n",
    "\n",
    "Because of that, in order to get the numbers right, it can be a good idea to pre-load it by allocating a tiny tensor on `cuda`. If we don't - the first experiment' stats will be misleading/incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#z = consume_gpu(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, if you use `IPyExperiments` - it performs this preloading for you already, when the backend is loaded (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with no GPU\n",
    "\n",
    "Let's consume a big chunk of non-GPU RAM and reclaim it at the end of the experiment.\n",
    "\n",
    "In this experiment we use the `cpu` backend, so GPU RAM will not be managed, regardless of whether there is a GPU that can be used or not. This mode is primarily used for configurations without GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Starting experiment...\n",
      "Backend: CPU-only\n",
      "\n",
      "*** Current state:\n",
      "RAM:   Used      Free     Total    Util\n",
      "CPU: 165.6 MB   17.2 GB  17.4 GB   0.94% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp1 = IPyExperimentsCPU() # consume some cpu ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = consume_cpu(2**14) # about 2GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = consume_cpu(2**14) # about 2GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Finishing experiment...\n",
      "\n",
      "*** Deleting the following local variables:\n",
      "['x1', 'x2']\n",
      "\n",
      "*** RAM consumed during the experiment:\n",
      "CPU: 4.0 GB\n",
      "\n",
      "*** RAM reclaimed at the end of the experiment:\n",
      "CPU: 4.0 GB (100.00%)\n",
      "\n",
      "*** Elapsed wallclock time:\n",
      "00:00:01\n",
      "\n",
      "*** Current state:\n",
      "RAM:   Used      Free     Total    Util\n",
      "CPU: 165.7 MB   17.2 GB  17.4 GB   0.94% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp1 # finish experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Experiment: consume general and GPU RAM\n",
    "\n",
    "Let's consume a big chunk of each, general and GPU RAM and reclaim both of them, at the end of the experiment.\n",
    "\n",
    "This time we wil use the GPU backed `pytorch`, so both GPU and general RAM will be managed. This is the default backed, so if you don't pass this argument, it'll default to `pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Starting experiment...\n",
      "Backend: Pytorch\n",
      "Device: ID 0, GeForce GTX 1070 Ti (7.9 GB RAM)\n",
      "\n",
      "*** Current state:\n",
      "RAM:   Used      Free     Total    Util\n",
      "CPU:   2.1 GB   15.5 GB  17.6 GB  13.59% \n",
      "GPU:   1.6 GB    6.3 GB   7.9 GB  25.83% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp2 = IPyExperimentsPytorch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = consume_cpu(2**14) # about 2GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = consume_gpu(2**14) # about 1GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Finishing experiment...\n",
      "\n",
      "*** Deleting the following local variables:\n",
      "['x1', 'x2']\n",
      "\n",
      "*** RAM consumed during the experiment:\n",
      "CPU: 2.0 GB\n",
      "GPU: 1.0 GB\n",
      "\n",
      "*** RAM reclaimed at the end of the experiment:\n",
      "CPU: 2.0 GB (99.98%)\n",
      "GPU: 1.0 GB (100.00%)\n",
      "\n",
      "*** Elapsed wallclock time:\n",
      "00:00:02\n",
      "\n",
      "*** Current state:\n",
      "RAM:   Used      Free     Total    Util\n",
      "CPU:   2.1 GB   15.5 GB  17.6 GB  13.60% \n",
      "GPU:   1.6 GB    6.3 GB   7.9 GB  25.83% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "del exp2 # finish experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get experiment data, preserve some vars\n",
    "\n",
    "Here we demonstate features that help with using this framework programmatically. i.e. getting the functions to return experiment data during and at the end of the experiment, rather than just printing it. You can then use it to programmatically refine the hyper parameters before rerunning the experiment.\n",
    "\n",
    "This experiment also demonstrates how to save some of the local variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Starting experiment...\n",
      "Backend: Pytorch\n",
      "Device: ID 0, GeForce GTX 1070 Ti (7.9 GB RAM)\n",
      "\n",
      "*** Current state:\n",
      "RAM:   Used      Free     Total    Util\n",
      "CPU:   2.1 GB   15.5 GB  17.6 GB  13.60% \n",
      "GPU:   1.6 GB    6.3 GB   7.9 GB  25.83% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp3 = IPyExperimentsPytorch() # consume some gpu and cpu ram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = consume_cpu(2**14) # about 1.5GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run an intermediary report of how much of the resources was consumed, and how much is available, returning the data as numbers. (none would be reclaimed yet, so it'll be zeros, but the return value is there for consistency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPyExperimentMemory(consumed=2147274752, reclaimed=0, available=14486392832) IPyExperimentMemory(consumed=0, reclaimed=0, available=6766002176)\n"
     ]
    }
   ],
   "source": [
    "cpu_data, gpu_data = exp3.data\n",
    "print(cpu_data, gpu_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preserve these variables, so that they remain available after the experiment is finished and the rest of the local variables get deleted. \n",
    "\n",
    "Note, that you need to pass the names of the variables and not the variables themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3.keep_var_names('cpu_data', 'gpu_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = consume_gpu(2**14) # about 1GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run another intermediary report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPyExperimentMemory(consumed=2147504128, reclaimed=0, available=14483275776) IPyExperimentMemory(consumed=1073741824, reclaimed=0, available=5692260352)\n",
      "2147504128\n",
      "5692260352\n"
     ]
    }
   ],
   "source": [
    "cpu_data, gpu_data = exp3.data\n",
    "print(cpu_data, gpu_data)\n",
    "print(cpu_data.consumed)\n",
    "print(gpu_data.available)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the experiment, delete local vars, reclaim memory, and run the final report of how much of the resources was consumed, and how much is available, and how much was reclaimed, returning the data as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Finishing experiment...\n",
      "\n",
      "*** Deleting the following local variables:\n",
      "['exp3', 'x1', 'x2']\n",
      "\n",
      "*** Keeping the following local variables:\n",
      "['cpu_data', 'gpu_data']\n",
      "\n",
      "*** RAM consumed during the experiment:\n",
      "CPU: 2.0 GB\n",
      "GPU: 1.0 GB\n",
      "\n",
      "*** RAM reclaimed at the end of the experiment:\n",
      "CPU: 2.0 GB (100.00%)\n",
      "GPU: 1.0 GB (100.00%)\n",
      "\n",
      "*** Elapsed wallclock time:\n",
      "00:00:01\n",
      "\n",
      "*** Current state:\n",
      "RAM:   Used      Free     Total    Util\n",
      "CPU:   2.1 GB   15.5 GB  17.6 GB  13.61% \n",
      "GPU:   1.6 GB    6.3 GB   7.9 GB  25.83% \n",
      "\n",
      "\n",
      "\n",
      "Numerical data:\n",
      " IPyExperimentMemory(consumed=2147504128, reclaimed=2147487744, available=16632782848) IPyExperimentMemory(consumed=1073741824, reclaimed=1073741824, available=6766002176)\n"
     ]
    }
   ],
   "source": [
    "cpu_data_final, gpu_data_final = exp3.finish() # finish experiment\n",
    "print(\"\\nNumerical data:\\n\", cpu_data_final, gpu_data_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's test that we can still access the variables we asked not to destroy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Half-way data:\n",
      " IPyExperimentMemory(consumed=2147504128, reclaimed=0, available=14483275776) IPyExperimentMemory(consumed=1073741824, reclaimed=0, available=5692260352)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nHalf-way data:\\n\", cpu_data, gpu_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the context manager\n",
    "\n",
    "If you want to put all cells into one, you could simplify the experiment even further by using its context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Starting experiment...\n",
      "Backend: Pytorch\n",
      "Device: ID 0, GeForce GTX 1070 Ti (7.9 GB RAM)\n",
      "\n",
      "*** Current state:\n",
      "RAM:   Used      Free     Total    Util\n",
      "CPU:   2.1 GB   15.5 GB  17.6 GB  13.60% \n",
      "GPU:   1.6 GB    6.3 GB   7.9 GB  25.83% \n",
      "\n",
      "\n",
      "\n",
      "*** Finishing experiment...\n",
      "\n",
      "*** Deleting the following local variables:\n",
      "['x1', 'x2']\n",
      "\n",
      "*** RAM consumed during the experiment:\n",
      "CPU: 2.0 GB\n",
      "GPU: 1.0 GB\n",
      "\n",
      "*** RAM reclaimed at the end of the experiment:\n",
      "CPU: 2.0 GB (100.00%)\n",
      "GPU: 1.0 GB (100.00%)\n",
      "\n",
      "*** Elapsed wallclock time:\n",
      "00:00:00\n",
      "\n",
      "*** Current state:\n",
      "RAM:   Used      Free     Total    Util\n",
      "CPU:   2.1 GB   15.5 GB  17.6 GB  13.61% \n",
      "GPU:   1.6 GB    6.3 GB   7.9 GB  25.83% \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with IPyExperimentsPytorch(): \n",
    "    x1 = consume_cpu(2**14)\n",
    "    x2 = consume_gpu(2**14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Starting experiment...\n",
      "Backend: Pytorch\n",
      "Device: ID 0, GeForce GTX 1070 Ti (7.9 GB RAM)\n",
      "\n",
      "*** Current state:\n",
      "RAM:   Used      Free     Total    Util\n",
      "CPU:   2.1 GB   15.5 GB  17.6 GB  13.60% \n",
      "GPU:   1.6 GB    6.3 GB   7.9 GB  25.83% \n",
      "\n",
      "\n",
      "\n",
      "*** Finishing experiment...\n",
      "\n",
      "*** Deleting the following local variables:\n",
      "['exp', 'x1', 'x2']\n",
      "\n",
      "*** Keeping the following local variables:\n",
      "['z']\n",
      "\n",
      "*** RAM consumed during the experiment:\n",
      "CPU: 2.0 GB\n",
      "GPU: 1.0 GB\n",
      "\n",
      "*** RAM reclaimed at the end of the experiment:\n",
      "CPU: 2.0 GB (100.00%)\n",
      "GPU: 1.0 GB (100.00%)\n",
      "\n",
      "*** Elapsed wallclock time:\n",
      "00:00:00\n",
      "\n",
      "*** Current state:\n",
      "RAM:   Used      Free     Total    Util\n",
      "CPU:   2.1 GB   15.5 GB  17.6 GB  13.61% \n",
      "GPU:   1.6 GB    6.3 GB   7.9 GB  25.83% \n",
      "\n",
      "\n",
      "some data\n"
     ]
    }
   ],
   "source": [
    "with IPyExperimentsPytorch() as exp: \n",
    "    x1 = consume_cpu(2**14)\n",
    "    z = \"some data\"\n",
    "    x2 = consume_gpu(2**14)\n",
    "    exp.keep_var_names('z')\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript # prevent committing an unsaved notebook\n",
    "IPython.notebook.save_notebook()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "285px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "323px",
    "left": "956px",
    "right": "20px",
    "top": "152px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
